\section{Coreset Algorithms}

Though the algorithms vary in details, coreset constructions come in one of the following two flavours:
\begin{enumerate}
\item Movement-based constructions: Such algorithms compute a clustering with $T$ points such that $\cost_A(T)\ll \opt$.
\chris{I do not get this. Shouldn't the algorithm create a coreset $S$ consisting of $T$ points and then compute $\cost_{S}(C)$ for some clustering $C$?}
The coreset guarantee then follows as a consequence of the triangle inequality. Popular examples from theory include~\cite{FrahlS2005,HaM04}. 

\item Importance sampling: Points are sampled proportionate to their sensitivity which for a point $p$ is defined as $sens(p):=\sup_{C} \frac{\min_{c\in C} \dist^2(p,c)}{\cost_A(C)}$ and weighted by their inverse sampling probability. In terms of theoretical performance, sensitivity sampling has largely replaced movement based constructions, see for example~\cite{FeldmanL11,LangbergS10}.  
\end{enumerate}

Of course, there exist algorithms that draw on techniques from both, see for example~\cite{Cohen-AddadSS21}. In what follows, we will survey implementations of various coreset constructions that we will evaluate later.



\begin{description}
\item[StreamKM++~\cite{AckermannMRSLS12}] The popular $k$-means++ algorithm~\cite{ArV07} computes a set of centers $K$ by iteratively sampling a point \chris{$p$ in $A$?} proportionate to $\min_{q\in K} \dist^2(p,q)$ and adding it to $K$. The procedure terminates once the desired number of centers has been reached. The first center is typically picked uniformly at random.
The StreamKM++ paper runs the $k$-means++ algorithms for $T$ iterations, where $T$ is the desired coreset size. At the end, every point $q$ in $K$ is weighted by the number of points in $A$ closest to it. While the construction has elements of important sampling, the analysis is largely movement-based.
\item[BICO~\cite{FGSSS13}] Combines the very fast, but poor quality clustering algorithm BIRCH~\cite{ZRL97} with the movement-based analysis from~\cite{FrahlS2005,HaM04}. The clustering is organized by way of a hierarchical decomposition: When adding a point $p$ to one of the coreset points $T$ at level $i$, it first finds the closest point $q$ in $T$. \chris{Would it be OK to use $S$ or $\Omega$ to denote coreset points because $T$ meant the coreset size in previous sections?} If $p$ is too far away from $q$, a new center is opened at $p$. Otherwise $p$ is either added to $q$, or, if adding $p$ to $q$ increases the clustering cost of $q$ beyond a certain threshold, the algorithm attempts to add $p$ to the child-clusters of $q$. The procedure then continues recursively. 
\item[Sensitivity Sampling~\cite{FL11}] The simplest implementation of sensitivity sampling first computes an $(O(1),O(1))$ bicriteria $K$ approximation\footnote{An $(\alpha,\beta)$ bicriteria approximation computes an $\alpha$ approximation using $\beta\cdot k$ many centers.}, for example by running $k$-means++ for $2k$ iterations~\cite{Wei16}. Let $K$ be the $2k$ clustering thus computed and let $K_i$ be an arbitrary cluster of $K$ with center $q_i$. Subsequently, the algorithm picks $T-2k$ points proportionate to $\frac{\dist^2(p,q)}{\cost_{K_i}(\{q_i\})} + \frac{1}{|K_i|}$. Let $|\hat{K_i}|$ be the estimated number of points in the sample. Finally, the algorithm weighs each $q_i$ by $(1+\eps)\cdot |K_i| - |\hat{K_i}|$.
\item[Group Sampling~\cite{Cohen-AddadSS21}] First, the algorithm computes an $O(1)$ approximation (or a bicriteria approximation) $K$. Subsequently, the algorithm preprocesses the input into groups such that (1) for any two points $p,p'\in K_i$, their cost is identical up to constant factors and (2) for any two clusters $K_i,K_j$, their cost is identical up to constant factors. In every group, Group-Sampling now samples points proportionate to their cost. The authors of~\cite{Cohen-AddadSS21} \chris{show? prove?} that there always exist a partition \chris{partitioning?} into $\log^2 1/\varepsilon$ groups. Points not contained in a group are snapped to their closest center $q$ in $K$. $q$ is weighted by the number of points snapped to it.
\end{description}

\subsection{Dimension Reduction}

Finally, we also combine coreset constructions with a variety of dimension reduction techniques. Since the seminal paper by Feldman, Schmidt, and Sohler~\cite{FSS13}, most coreset algorithms have used some form of dimension reduction to eliminate the dependency on $d$, either by explicitly computing a low-dimensional embedding, see for example~\cite{FSS13,SoW18}, or by using the existence of a suitable embedding in the analysis~\cite{Cohen-AddadSS21,huang2020coresets}.

In particular, movement-based coresets often have an exponential dependency on the dimension, which can be alleviated with some form of dimension reduction, both in theory~\cite{SSS19} and in practice~\cite{KappmeierS015}.

Here the are two main techniques.

\begin{description}
\item[Principal Component Analysis:] Feldman, Schmidt, and Sohler~\cite{FSS13} showed that projecting an input $A$ onto the first $O(k/\varepsilon^2)$ principal components is a coreset, albeit in low dimension. The analysis was subsequently tightened by~\cite{CEMMP15} and extended to other center based cost functions by~\cite{SohlerW18}. Although it's target dimension is generally worse than those based on random projections and terminal embeddings, there is nevertheless reasons for using PCA regardless: It removes noise and thus may make it easier to compute a high quality coreset.
\item[Terminal Embeddings:] Given a set of points $A$ in $\mathbb{R}^d$, a terminal embedding $f:\mathbb{R}^d\rightarrow \mathbb{R}^m$ preserves the pairwise distance between any point $p\in A$ and any point $q\in \mathbb{R}^d$ up to a $(1\pm \varepsilon)$ factor. The statement is related to the famous Johnson-Lindenstrauss lemma but it is stronger as it does not apply to only the pairwise distances of $A$. Nevertheless, the same target dimension is sufficient. Terminal embeddings were studied by~\cite{ElkinFN17,MahabadiMMR18,NaN18}, with Narayanan and Nelson \cite{NaN18} achieving an optimal target dimension of $O(\varepsilon^{-2}\log n)$, where $n$ is the number of points. For applications to coresets, we refer to \cite{BecchettiBC0S19,Cohen-AddadSS21,huang2020coresets}.
\end{description}

For an overview on practical aspects of dimension reduction, we refer to Venkatsubramanian and Wang~\cite{VenkatasubramanianW11}.