\section{Conclusion} \label{sec:conclusion}
In this work, we studied how to assess the quality of $k$-means coresets computed by state-of-the-art algorithms. It is generally hard to measure the quality of a coreset because it requires computing the worst-case distortion. Due to this difficulty, earlier works evaluated coresets by the outcome of an optimization algorithm. This method of comparison has the drawback that it is more likely to measure the performance of the underlying optimization problem, rather than evaluating coresets. As a alternative, we proposed a new evaluation procedure which can estimate the quality of coresets on real-world data sets. To complement this, we also proposed a benchmark framework which provably generates hard instances for all known $k$-means coreset algorithms. We evaluated the quality of five $k$-means coreset algorithms on five real-world datasets and four instances of the proposed benchmark. We experimented with both movement-based and sampling-based coreset algorithms. We found that while all algorithms produce coresets which yield similar low cost clusterings, sampling-based methods are superior to movement-based algorithms. Thus, unless there are important secondary reasons for employing movement based algorithms such as difficulty of accessing random bits or running time considerations, sampling based methods are preferable in practise.


Surprisingly, the quality of the coresets produced by StreamKM++ is significantly better than what one would expect from its theoretical analysis. StreamKM++ is derived from a movement-based construction similar to BICO. Emprically, StreamKM++ is notably better than BICO across all data sets, and especially on high dimensional data. This begs the question whether there exist a better theoretical analysis for StreamKM++. We leave this as an open problem for future research.
