\section{Conclusion} \label{sec:conclusion}
In this work, we studied the evaluation of coreset algorithms for the $k$-means problem. In general, it is hard to measure the worst-case distortion of coresets. We showed that verifying coresets is co-NP-hard. As a practical alternative, we proposed a new evaluation procedure on real-world data set. To complement the evaluation of coreset algorithms, we also proposed a benchmark framework which provably generates hard instances for $k$-means coresets. 
% An instance consists of a large number of clusterings which have the same cost but are very different in composition. By verifying the distortions of these known clusterings, one can estimate the distortion of any given coreset.
We evaluated the quality of five $k$-means coreset algorithms on five real-world datasets and four instances of our proposed benchmark. We evaluated two practical state-of-the-art algorithms namely BICO and StreamKM++, the two older algorithms Ray Maker and Sensitivy Sampling, and the new contender Group Sampling. In general, Ray Maker and BICO perform the worst while Group Sampling and Sensitivy Sampling perform the best. StreamKM++ also performs very well coming in close second after Group Sampling and Sensitivy Sampling.
