\section{Conclusion} \label{sec:conclusion}
In this work, we studied the evaluation of coreset algorithms for the $k$-means problem. In general, it is hard to measure the worst-case distortion of coresets. We showed that verifying coresets is co-NP-hard. As a practical alternative, we proposed a benchmark framework which provably generates hard instances for $k$-means coresets. An instance consists of a large number of clusterings which have the same cost but are very different in composition. By verifying the distortions of these known clusterings, one can estimate the distortion of any given coreset.

We evaluated the quality of five $k$-means coreset algorithms on five real-world datasets and our proposed benchmark. We evaluated two practical state-of-the-art algorithms namely BICO and StreamKM++, two older algorithms Ray Maker and Sensitivy Sampling, and a new contender Group Sampling. In general, Ray Maker and BICO perform the worst while Group Sampling and Sensitivy Sampling perform the best. StreamKM++ also performs very well coming in close second after Group Sampling and Sensitivy Sampling.
