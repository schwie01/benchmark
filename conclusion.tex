\section{Conclusion} \label{sec:conclusion}
In this work, we studied how to assess the quality of $k$-means coresets computed by state-of-the-art algorithms. It is generally hard to measure the quality of a coreset because it requires computing the worst-case distortion. Due to this difficulty, earlier works evaluated coresets by the outcome of an optimization algorithm. This method of comparison has the drawback that it is more likely to measure the performance of the underlying optimization problem, rather than evaluating coresets. As a alternative, we proposed a new evaluation procedure which can estimate the quality of coresets on real-world data sets. To complement this, we also proposed a benchmark framework which provably generates hard instances for all known $k$-means coreset algorithms. We evaluated the quality of five $k$-means coreset algorithms on five real-world datasets and four instances of the proposed benchmark. We experimented with both movement-based and sampling-based coreset algorithms. We found that while all algorithms produce coresets which yield similar low cost clusterings, sampling-based methods are superior to movement-based algorithms. 
