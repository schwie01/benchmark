\section{Conclusion} \label{sec:conclusion}
In this work, we studied how to assess the quality of $k$-means coresets computed by state-of-the-art algorithms. It is generally hard to measure the quality of a coreset because it requires computing the worst-case distortion. Due to this difficulty, earlier works evaluated coresets by the outcome of an optimization algorithm. This method of comparison has the drawback that it is more likely to measure the performance of the underlying optimization problem, rather than evaluating coresets. As a alternative, we proposed a new evaluation procedure which can estimate the quality of coresets on real-world data sets. To complement this, we also proposed a benchmark framework which provably generates hard instances for all known $k$-means coreset algorithms. We evaluated the quality of five $k$-means coreset algorithms on five real-world datasets and four instances of the proposed benchmark. We experimented with both movement-based and sampling-based coreset algorithms. We found that while all algorithms produce coresets which yield similar low cost clusterings, sampling-based methods are superior to movement-based algorithms. 
\chris{Add the k-means++ question here. I'm not sure if I ever mentioned that before. If you came up with it by yourself, I am very impressed. It also flowed naturally in what you were writing.}
\omar{I wish I came up with it myself! I do not yet understand the proofs. Anyway, I moved the section ``BICO versus StreamKM++'' to here and tried to make it shorter. What do you think?}

Surprisingly, the quality of the coresets produced by StreamKM++ is significantly better than what one would expect from its theoretical analysis. StreamKM++ is derived from a movement-based construction similar to BICO. Emprically, StreamKM++ is notably better than BICO across all data sets, and especially on high dimensional data. This begs the question whether there exist a better theoretical analysis for StreamKM++. We leave this as an open problem for future research.
