


\section{Experiments} \label{sec:experiments}
In this section, we present how we evaluated different algorithms. First, we propose a new evaluation procedure which gauges the quality of coresets. Then, we describe the data sets used for the empirical evaluation and our experimental setup. Finally, we detail the outcome of the experiments and our observations on the results.

\subsection{Evaluation Procedure}
\omar{Removed subsection ``Motivation for Introducing New Evaluation Procedure'' and wrote Evaluation Procedure in a slightly different way. What do you think?}
Finding a $k$-clustering which results in a maximal distortion is difficult. Instead, we can estimate the quality of coresets on real-world data sets as follows: we first compute a coreset $\Omega$. Then, run $k$-means++ on $\Omega$ to find a set of $k$ centers $C$. We repeat the $k$-means++ algorithm 5 times to pick the set of centers with the largest distortion.

While it is useful to evaluate coresets on real-world data sets, it can be tricky to gauge the general performance of coreset algorithms using only a small selection of data sets. For this reason, we used our benchmark to complement the evaluation on real-world data sets. The benchmark accomplishes two important tasks. First, the benchmark allows us to quickly find a bad solution because both good and bad clusterings are known a priori. It is unclear how to find bad clusterings for real-world data sets. Second, it is easier to make a fair comparison of different coreset constructions because the benchmark is known to generate hard instances for all known coreset algorithms. This cannot be said for real-world data sets. For the benchmark, we computed the distortion following the evaluation procedure described in~\cref{sec:benchmark}. 
\omar{The paragraph above can probably be reduced to two sentence about how we use the benchark, but I wanted to emphasize why the benchmark is so useful. I am okay if it gets removed due to lack of space.}

Every randomized coreset construction was repeated $10$ times. We aggregated the reported distortions by taking the maximum over all $10$ evaluations. 
\chris{I don't get this. Did you take $D = \frac{1}{10}\sum D_i$, where $D_i$ is the maximum distortion in the $i$th evaluation or $D=\max D_i$? Because I would understand using the mean (or median), but not so much using the max both times. We repeat the experiment to filter out outliers and bad runs; using the max means we always take the biggest outlier.}
\omar{I did the later, meaning I took the maximum distortion over all 10 repetitions of an experiment. This is what I do exact.
\newline\newline
For each iteration $i \in \{1, 2, \cdots, 10\}$ of an experiment:
\begin{enumerate}
    \item Compute a coreset $\Omega_i$ on $A$
    \item Run $k$-means++ algorithm 5 times. \\Each run of  $k$-means++ on $\Omega_i$ outputs a clustering $C_{ij}$
    \item Then, the best lower bound on the distortion fir $\Omega_i$ is the solution with the largest distortion:
    $$
    D_i =
    \max_j\left(
    \max\left(
      \frac{\cost_A(C_{ij})}{\cost_{\Omega_i}(C_{ij})},
      \frac{\cost_{\Omega_i}(C_{ij})}{\cost_A(C_{ij})}
    \right)
    \right)
    $$
\end{enumerate}
Having computed 10 distortions, I pick $D=\max D_i$ because it captures the worst possible distortion that an algorithm can produce. Since the coreset definition does not say anything about an algorithm generating a coreset with a certain distortion with high probability, I assumed that a ``bad'' run with extremely high distortion should give an estimate of the general quality of the evaluated coreset construction. The idea is that an outlier is not a failure mode of the algorithm but an indication of how good it actually is. Not sure if this makes sense. What do you think?
\newline\newline
In any case, I computed the the average distortions with variances over the 10 runs. I created an appendix with these results, see \cref{tab:distortions-mean-std-without-pca} and \cref{tab:distortions-mean-std-with-pca}. I also regenerated the plots so they now show the average distortion $D = \frac{1}{10}\sum D_i$.
}
In addition, we preprocessed the data using the dimension reduction techniques described in Section~\ref{sec:algorithms}.


% We now present the empirical evaluation of these coresets.
% We ran two kinds of experiments. On real-world data sets, we merely computed a coreset $\Omega$, followed by running $k$-means++ on $\Omega$. 
% The $k$-means++ algorithm was repeated 5 times, each yielding a solution $\calS_i$, and as the best lower bound on the distortion we used the largest ratio $\max_i\left(\max\left(\frac{\cost_A(\calS_i)}{\cost_{\Omega}(\calS_i)},\frac{\cost_{\Omega}(\calS_i)}{\cost_A(\calS_i)}\right)\right)$.
% For the benchmark, we used the evaluation as proposed in Section~\ref{sec:benchmark}. In addition, we also determined the distortion via simply running the $k$-means++ algorithm. 

% Except for BICO, which is deterministic, this experiment was repeated for each coreset algorithm $10$ times. \omar{Experiments were repeated 10 times including BICO. Although vanilla BICO is deterministic, we used BICO with heuristic speed optimizations which are stochastic.} We aggregated the reported distortions by taking the maximum over all $10$ evaluations. In addition, we also preprocessed the data using the dimension reduction techniques described in Section~\ref{sec:algorithms}.


\subsection{Data sets}
We conducted experiments on five real-world data sets and four instances of our benchmark. The sizes of the real-world data sets are summarized in ~\cref{tab:real-world-datasets-overview}. Benchmark instances were generated to match approximately the sizes of the real-world data sets. The chosen parameter values and the corresponding instance sizes are shown in ~\cref{tab:benchmark-instances-overview}. 
% We generated a set of instances with no scaling i.e., $\beta=1.0$ (referred to as \textit{Benchmark-1.0}) and with maximum scaling; $\beta = 2.0$ (\textit{Benchmark-2.0}).
We now provide a brief description of each of the real-world data sets.

The \textit{Census}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990)}} dataset is a small subset of the Public Use Microdata Samples from 1990 US census. It consists of demographic information encoded as 68 categorical attributes of 2,458,285 individuals. 

\textit{Covertype}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/covertype}} is comprised of cartographic descriptions and forest cover type of four wilderness areas in the Roosevelt National Forest of Northern Colorado in the US. It consists of 581,012 records, 54 cartographic variables and one class variable. Although \textit{Covertype} was originally made for classification tasks, it is often used for clustering tasks by removing the class variable~\cite{AckermannMRSLS12}.

The data set with the fewest number of dimensions is \textit{Tower}\footnote{\url{http://homepages.uni-paderborn.de/frahling/coremeans.html}}. This data set consists of 4,915,200 rows and 3 features as it is a 2,560 by 1,920 picture of a tower on a hill where each pixel is represented by a RGB color value. 

% We used the datasets \textit{Census}, \textit{Covertype} and \textit{Tower} as these are often used to evaluate the performance of coreset algorithms. 

% To include larger datasets in evaluation, we used \textit{Caltech} and \textit{NYTimes}. 

Inspired by~\cite{FGSSS13}, \textit{Caltech} was created by computing SIFT features from the images in the Caltech101\footnote{\url{http://www.vision.caltech.edu/Image_Datasets/Caltech101/}} image database. This database contains pictures of objects partitioned into 101 categories. Disregarding the categories, we concatenated the 128-dimensional SIFT vectors from each image into one large data matrix with 3,680,458 rows and 128 columns. 

% \textit{NYTimes}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/bag+of+words}} dataset in our experiments as the number of dimensions is very large.
\textit{NYTimes}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/bag+of+words}} is a dataset composed of the bag-of-words (BOW) representations of 300,000 news articles from The New York Times. The vocabulary size of the text collection is 102,660. Due to the BOW encoding, \textit{NYTimes} has a very large number of dimensions and is highly sparse. To make processing feasible, we reduced the number of dimensions to 100 using terminal embeddings.

\subsection{Preprocessing \& Experimental Setup}
To understand how denoising effects the quality of the outputted coresets, we applied Principal Component Analysis (PCA) on \textit{Caltech}, \textit{Census} and \textit{Covertype} by using the $k$ singular vectors corresponding to the largest singular values. For these three data sets, we preserved the dimensions of the original data.  
The \textit{NYTimes} dataset did not permit the preservation of dimensions as the number of dimensions is very large. In this case, we used PCA to reduce the dimensions to $k$.
We did not perform any preprocessing on \textit{Tower} due to its low dimensionality.

We followed the same experimental procedure with respect to the choice of parameter values for the algorithms as prior works~\cite{AckermannMRSLS12, FGSSS13}. For the target coreset size, we used $200k$ for all our experiments. On \textit{Caltech}, \textit{Census},  \textit{Covertype} and \textit{NYTimes}, we used $k$ values in $\{10, 20, 30, 40, 50\}$, while for \textit{Tower} we used larger cluster sizes $k \in \{20, 40, 60, 80, 100\}$. On the benchmark instances, we settled on $k \in \{10, 20, 30, 40\}$ as a reasonable trade-off between running time and data set size.


We implemented Sensitivity Sampling, Group Sampling, Ray Maker, and StreamKM++ in C++. The source code can be found on GitHub\footnote{Link to repository will be provided later.}. For BICO, we used the authors' reference implementation\footnote{\url{https://ls2-www.cs.tu-dortmund.de/grav/en/bico}}. The source code was compiled with gcc 9.3.0. The experiments were performed on a machine with 14 cores (3.3 GHz) and 256 GB of memory.





%
\begin{table}
	\begin{center}%\centering
	\caption{The sizes of the real-world data sets used for the experimental evaluation}
	\label{tab:real-world-datasets-overview}
% 	\resizebox{\textwidth}{!}{
	\begin{tabular}{lrr}
		\toprule
        
		    & Data points
		    & Dimensions
            \\
		\midrule
		\textit{Caltech}
    		& 3,680,458
    		& 128
    		\\
		\textit{Census}
    		& 2,458,285
    		& 68
    		\\
	    \textit{Covertype}
    	    & 581,012
    		& 54
    		\\
	    \textit{NYTimes}
    	    & 500,000
    		& 102,660
    		\\
        \textit{Tower}
            & 4,915,200
    		& 3
    		\\
		\bottomrule
	\end{tabular}\\
	\end{center}
% 	}
\end{table}



%
\begin{table}
	\begin{center}%\centering
	\caption{The parameter values and the sizes of the benchmark instances used for the experimental evaluation.}
	\label{tab:benchmark-instances-overview}
% 	\resizebox{\textwidth}{!}{
	\begin{tabular}{rrrr}
		\toprule
        $k$
		    & $\alpha$
		    & Data points
		    & Dimensions
            \\
		\midrule
        10
    		& 6
    		& 1,000,000
    		& 60
    		\\
        20
    		& 5
    		& 3,200,000
    		& 100
    		\\
        30
    		& 4
    		& 810,000
    		& 120
    		\\
        40
    		& 4
    		& 2,560,000
    		& 160
    		\\
    %     50
    % 		& 4
    % 		& 6,250,000
    % 		& 200
    % 		\\
		\bottomrule
	\end{tabular}\\
	\end{center}
% 	}
\end{table}



\begin{figure*}
  \caption{The distortions of the evaluated coreset algorithms on five real-world data sets and on four benchmark instances. The axis is non-linear as otherwise the bars for Sensitivity Sampling and Group Sampling would disappear on the plots. Their distortions are very close to 1.}
  \label{fig:distortions}
  \includegraphics[width=.69\linewidth]{figures/distortions-mean-Tower.pdf}
  \newline \newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/distortions-mean-Covertype.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/distortions-mean-Covertype+PCA.pdf}
  }
  \newline \newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/distortions-mean-Census.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/distortions-mean-Census+PCA.pdf}
  }
  \newline \newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/distortions-mean-Caltech.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/distortions-mean-Caltech+PCA.pdf}
  }
  \newline \newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/distortions-mean-NYTimes.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/distortions-mean-NYTimes+PCA.pdf}
  }
  \newline \newline
%   \subfloat{
%     \\[1ex]
%     \includegraphics[width=0.5\textwidth]{figures/distortions-mean-Benchmark.pdf}
%   }
  \subfloat{
    \includegraphics[width=0.165\textwidth]{figures/distortions-mean-Benchmark-k10.pdf}
    \includegraphics[width=0.165\textwidth]{figures/distortions-mean-Benchmark-k20.pdf}
    \includegraphics[width=0.165\textwidth]{figures/distortions-mean-Benchmark-k30.pdf}
    \includegraphics[width=0.331\textwidth]{figures/distortions-mean-Benchmark-k40.pdf}
  }
%   \subfloat{
%     \includegraphics[width=.5\linewidth]{figures/boxplot-Benchmark-GS-SS-StreamKM.pdf}
%   }
  
\end{figure*}


\begin{figure*}
  \caption{The average costs of running the evaluated coreset algorithms multiple times on different data sets. In general, the five coreset algorithms are able to compute coresets which result in solutions with comparable costs on the different real-world data sets. The differences in cost is more noticeable on the benchmark instances. Here, Senstivity Sampling is the winner because it seems to be better at capturing the correct ``clusters'' inherent in the benchmark instances.}
  \label{fig:real-costs}
  \includegraphics[width=.67\linewidth]{figures/real-costs-Tower.pdf}
  \newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/real-costs-Covertype.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/real-costs-Covertype+PCA.pdf}
  }
  \newline\newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/real-costs-Census.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/real-costs-Census+PCA.pdf}
  }
  \newline\newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/real-costs-Caltech.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/real-costs-Caltech+PCA.pdf}
  }
  \newline\newline
  \subfloat{
    \includegraphics[width=0.5\textwidth]{figures/real-costs-NYTimes.pdf}
  }
  \subfloat{
    \includegraphics[width=.5\linewidth]{figures/real-costs-NYTimes+PCA.pdf}
  }
  \newline\newline
  \subfloat{
    \includegraphics[width=0.15\textwidth]{figures/real-costs-Benchmark-k10.pdf}
    \includegraphics[width=0.165\textwidth]{figures/real-costs-Benchmark-k20.pdf}
    \includegraphics[width=0.16\textwidth]{figures/real-costs-Benchmark-k30.pdf}
    \includegraphics[width=0.31\textwidth]{figures/real-costs-Benchmark-k40.pdf}
  }
\end{figure*}





\subsection{Outcome of Experiments}
We summarized the distortions in \cref{fig:distortions}.
All five algorithms are matched on the \textit{Tower} dataset. The worst distortions across the algorithms are close to 1, and performance between the algorithms is negligible. The performance difference between sampling-based and movement-based methods become more pronounced as the number of dimensions increase. On \textit{Covertype} with its 54 features, Ray Maker performs the worst followed by BICO and Group Sampling while Sensitivity Sampling and StreamKM++ perform the best. Differences in performance are more noticeable on \textit{Census}, \textit{Caltech}, and \textit{NYTimes}  where methods based on importance sampling perform much better. Sensitivity Sampling and Group Sampling perform the best, StreamKM++ come in second while BICO and Ray Maker perform the worst across these data sets.
On the \textit{Benchmark}, Ray Maker is the worst while Sensitivty Sampling and Group Sampling are the best. StreamKM++ performs also very well compared to BICO.

Reducing noise with PCA helped boost the performance on real-world data sets with large number of dimensions. On \textit{Covertype}, PCA does not change the performance numbers by much. On \textit{Census} ($d=68$), the preprocessing step with PCA improves the performance of BICO and Ray Maker slightly for lower values of $k$. The distortions of BICO and Ray Maker are reduced markedly on \textit{Caltech} ($d=128$) after applying PCA. 


\subsection{Interpretation of Experimental Results}
\chris{Can we have one central take-away for each? I wrote a suggestion, let me know how you feel about them}
\omar{Yes! See my notes below.}


\subsubsection*{Optimization versus Compression}
For some data sets, optimization is very close to compression. It is not uncommon for the $k$-means cost of real-world data sets to drop significantly for larger values of $k$.
~\cref{fig:cost-curves-real-world-datasets} illustrates this behavior for several real-world data sets. The more the curve bends, the less of a difference there is between computing a coreset and a clustering with low cost. For data sets with a L-shaped cost curve, a coreset algorithm adding more centers to the coreset will seem to be performing well when evaluating it based on the outcome of the optimization.
\textit{Tower} is a good example of a data set where optimization is very close to compression. Its cost curve bends the most which indicates that adding more centers help reduce the cost. One of the strengths of the benchmark is that there is no way of reducing the cost without capturing the right subclusters within a benchmark instance. This means that the cost does not decrease markedly beyond a certain value of $k$ even if more centers are added as depicted in~\cref{fig:cost-curves-benchmark}. 

For BICO, Ray Maker, and StreamKM++, there is a correlation between the steepness of the cost curve for a data set and the distortion of the generated coreset. 
\chris{excellent. please add the aforementioned stuff here (plots and the parts from the paragraph that we will probably erase).}
\omar{Added at the beginning of the subsection for better text flow.}
For data sets where the curve is less steep, we observed higher distortions. The effect is more pronounced for the movement-based algorithms (BICO and Ray Maker) than for StreamKM++ which performs sampling. The other two sampling-based approaches (Group Sampling and Sensitivity Sampling) seem to be free from this behavior as they consistently generate high quality coresets irrespective of the shape of cost curve.

\chris{The aforementioned correlation between cost and distortion for movement and the fact that sampling seems to be rather oblivious to this.}
\omar{I agree, this take-away seems to be clear from the text.}






\begin{figure}
  \caption{Depicts how clustering costs of five real-world data sets decrease as the number of centers increase. 
  The most widely used data sets for evaluating coresets are \textit{Tower}, \textit{Covertype}, and \textit{Census}, while \textit{Caltech} is rarely used and \textit{NYTimes} has never been used before.
  Plotting the cost curve allows us to study whether we can observe a difference between coreset construction and optimization in a data set when evaluating a coreset based on cost.
  }
  \label{fig:cost-curves-real-world-datasets}
  \includegraphics[width=1\linewidth]{figures/cost-curves-real-world-datasets.pdf}
\end{figure}


\begin{figure}
  \caption{Shows the clustering costs of four instances of the benchmark framework as a function of the number of centers. In contrast to real-world data sets, the costs do not decrease rapidly as more cluster centers are added.
%   On this account, the benchmark is less sensitive to 
  }
  \label{fig:cost-curves-benchmark}
  \includegraphics[width=1\linewidth]{figures/cost-curves-benchmark.pdf}
\end{figure}


\subsubsection*{Movement-based versus Sampling-based Approaches}
While all five algorithms are equally matched when comparing clustering costs (see  \cref{fig:real-costs}), coreset quality performance differ significantly (see \cref{fig:distortions}). 
In general, movement-based constructions perform the worst in terms of coreset quality. 
We observed that BICO and Ray Maker have the highest distortions across all data sets including on the benchmark instances. Among the sampling-based algorithms, Sensitive Sampling performs well with Group Sampling generally being competitive. StreamKM++ is an interesting case. Like the movement-based methods, its distortion increases with the dimension. Nevertheless, it generally performs significantly better than BICO and Ray Maker. This can be attributed to the fact that the coreset produced by StreamKM++ consists entirely of $k$-means++ centers weighted by the number of points of a minimal cost assignment. This is similar to movement-based algorithms such as BICO. Nevertheless, it also retains some of the performance from pure importance schemes.
\chris{Can we say something about the impact of dimension? I am not sure I read the plot right (the non-linearity of the axis makes it a bit difficult), but is the distortion of the sampling based stuff affected by the dimension, or more or less independent? Also, I think we want to highlight more one key take away message: Dimension affects distortion for movement based in practise, not just in theory.}
\omar{Yes, we can say something about the impact of dimension. Please see \cref{tab:distortions-mean-std-with-pca} for the distortion numbers. What do you think of adding the following: }
In practice as well as in theory, the distortion of movement-based algorithms are affected by the dimension. By comparison, sampling-based algorithms are affected very little by the dimension. The distortion of StreamKM++ increase only slightly with the dimension while Group Sampling and Sensitivity Sampling are largely unaffected.





\subsubsection*{Impact of PCA}

\begin{figure}
  \caption{The cumulative propotion of explained variance by principal components on \textit{Caltech}, \textit{Covertype}, and \textit{Census}.}
  \label{fig:explained-variance-pca}
  \includegraphics[width=0.9\linewidth]{figures/explained-variance-plot.pdf}
\end{figure}

On almost all our data sets, the performance improves when input data is preprocessed with PCA, especially for the movement-based algorithms. Empirically, the more noise is removed (i.e., small $k$ value), the lower the distortion. Notice that $k$ is the number of principal components that the input data is projected on to. The rest of the low variance components are treated as noise and removed. Method utilizing sampling (Group Sampling, Sensitivity Sampling and StreamKM++) are less effected by the preprocessing step. On \textit{Covertype}, PCA does not change the distortions by much because almost all the variance in the data is explained by the first five principal components (see~\cref{fig:explained-variance-pca}). 
On \textit{Caltech} and \textit{NYTimes}, the quality of the coresets by BICO and Ray Maker improves greatly because the noise removal is more aggressive. Even if the quality is much better for movement-based coreset constructions due to PCA transformation, importance sampling methods are still superior when it comes to the quality of the compression. 
\chris{Take away message at the end: All coreset constructions are affected by the dimension (even sampling, though markedly less so). We can say, PCA is computationally very expensive, but may be nevertheless be worth it.}
\omar{Agreed, that is a concluding message!}

